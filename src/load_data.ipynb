{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1597cf86",
   "metadata": {},
   "source": [
    "### Loading data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b5e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['polarity of tweet ', 'id of the tweet', 'date of the tweet', 'query',\n",
      "       'user', 'text of the tweet '],\n",
      "      dtype='object')\n",
      "Index(['polarity of tweet', 'id of the tweet', 'date of the tweet', 'query',\n",
      "       'user', 'text of the tweet'],\n",
      "      dtype='object')\n",
      "Total samples: 1048572\n",
      "Sample tweet: is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#path of the dataset\n",
    "DATA_PATH = \"../data/raw/training.1600000.processed.noemoticon.csv\"\n",
    "# columns = [\"polarity\",\"id\",\"date\",\"query\",\"user\",\"text\"]\n",
    "\n",
    "#load dataset\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    encoding=\"latin-1\"        # twitter texts contains imogies so UTF-8 may crash \n",
    "    # header=None,            # file has no header\n",
    "    # names=columns           # assigning my own header\n",
    ")\n",
    "print(df.columns) # ['polarity of tweet ', 'id of the tweet', 'date of the tweet', 'query','user', 'text of the tweet ']\n",
    "df.columns = df.columns.str.strip()\n",
    "print(df.columns) # ['polarity of tweet', 'id of the tweet', 'date of the tweet', 'query','user', 'text of the tweet']\n",
    "\n",
    "\n",
    "#extract x and y\n",
    "# x = df[\"text\"]\n",
    "# y = df[\"polarity\"]\n",
    "x = df[\"text of the tweet\"]\n",
    "y = df[\"polarity of tweet\"]\n",
    "\n",
    "# 0->0 and 4->1\n",
    "y = y.map({0: 0, 4: 1})\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"Sample tweet:\", x.iloc[0])\n",
    "print(\"Sample label:\", y.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72e501",
   "metadata": {},
   "source": [
    "### Data Preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228ee7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "Cleaned  : is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n"
     ]
    }
   ],
   "source": [
    "import re # regular expression (It lets you search, match, extract, and replace text patterns in strings.)\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)     # remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)        # remove mentions\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)    # keep only letters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text of the tweet\"].apply(clean_tweet) # here I have created a new column with header clean_text\n",
    "\n",
    "# sanity check\n",
    "print(\"Original :\", df[\"text of the tweet\"].iloc[0])\n",
    "print(\"Cleaned  :\", df[\"clean_text\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cc329",
   "metadata": {},
   "source": [
    "Check if vertual environment is used or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45cd7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/midori/Desktop/sentiment_analysis/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7547e6b",
   "metadata": {},
   "source": [
    "### Tokenize processed text (GLOBAL PATH SETUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc0a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "NLTK_DATA_DIR = os.path.join(PROJECT_ROOT, \"nltk_data\")\n",
    "\n",
    "# Ensure NLTK looks here first\n",
    "nltk.data.path.insert(0, NLTK_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9edcfa",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0cd904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text : is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n",
      "Tokens: ['is', 'upset', 'that', 'he', 'cant', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', 'blah']\n"
     ]
    }
   ],
   "source": [
    "resources = [\n",
    "    (\"tokenizers/punkt\", \"punkt\"),\n",
    "    (\"tokenizers/punkt_tab/english\", \"punkt_tab\"),\n",
    "]\n",
    "\n",
    "for path, name in resources:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(name, download_dir=NLTK_DATA_DIR)\n",
    "\n",
    "# Tokenization\n",
    "# df[\"tokens\"] = df[\"clean_text\"].apply(word_tokenize)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Cleaned text :\", df[\"clean_text\"].iloc[0])\n",
    "print(\"Tokens:\", df[\"tokens\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95225f21",
   "metadata": {},
   "source": [
    "### Remove common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91474f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before : ['is', 'upset', 'that', 'he', 'cant', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', 'blah']\n",
      "Tokens after  : ['upset', 'cant', 'update', 'facebook', 'texting', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpora = [\n",
    "    (\"corpora/stopwords\", \"stopwords\"),\n",
    "]\n",
    "\n",
    "for rel_path, name in corpora:\n",
    "    expected_path = os.path.join(NLTK_DATA_DIR, rel_path)\n",
    "    if not os.path.exists(expected_path):\n",
    "        nltk.download(name, download_dir=NLTK_DATA_DIR)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"tokens_nostop\"] = df[\"tokens\"].apply(\n",
    "    lambda tokens: [w for w in tokens if w not in stop_words]\n",
    ")\n",
    "\n",
    "# # sanity check\n",
    "print(\"Tokens before :\", df[\"tokens\"].iloc[0])\n",
    "print(\"Tokens after  :\", df[\"tokens_nostop\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4016b72",
   "metadata": {},
   "source": [
    "### TF-IDF  (Term frequency - Inverse Document Frequency)\n",
    "TF-IDF turns each tweet into a fixed-length numeric vector where each value represents how important a word is in that tweet compared to all tweets.\n",
    "\n",
    "* `X` → numeric feature matrix (THIS is what ML uses)\n",
    "* `y` → sentiment labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee1c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (1048572, 15000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",   # built-in stopwords\n",
    "    max_features=15000       # limit vocabulary size (important for memory)\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976de846",
   "metadata": {},
   "source": [
    "Check sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1d446b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "0.0003844256760623019\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.nnz / (X.shape[0] * X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e088e5c",
   "metadata": {},
   "source": [
    "### train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "686acd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (838857, 15000)\n",
      "X_test shape : (209715, 15000)\n",
      "y_train shape: (838857,)\n",
      "y_test shape : (209715,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,      # 80% train, 20% test\n",
    "    random_state=42,    # reproducibility\n",
    "    stratify=y          # keep class balance\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b90cc0",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5c7ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/midori/Desktop/sentiment_analysis/venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=1300,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3705470",
   "metadata": {},
   "source": [
    "### Saved the logistic-regression model and vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a876839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer and model saved successfully\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(vectorizer, os.path.join(MODEL_DIR, \"tfidf_vectorizer.joblib\"))\n",
    "dump(model, os.path.join(MODEL_DIR, \"logistic_regression.joblib\"))\n",
    "\n",
    "print(\"Vectorizer and model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c1a78e",
   "metadata": {},
   "source": [
    "### Test the model (Prediction + Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a73a937f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8311470328779534\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90    160000\n",
      "           1       0.72      0.47      0.57     49715\n",
      "\n",
      "    accuracy                           0.83    209715\n",
      "   macro avg       0.79      0.71      0.73    209715\n",
      "weighted avg       0.82      0.83      0.82    209715\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[151090   8910]\n",
      " [ 26501  23214]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
